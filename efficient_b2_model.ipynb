{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sidhtang/vehicle-detection-/blob/main/efficient_b2_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IM0WVXBnvdKd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics colorthief deep_sort_realtime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Mefo1jXDOVrg",
        "outputId": "0ebecc04-510f-4d66-9848-91803cade78b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.59-py3-none-any.whl.metadata (35 kB)\n",
            "Collecting colorthief\n",
            "  Downloading colorthief-0.2.1-py2.py3-none-any.whl.metadata (816 bytes)\n",
            "Collecting deep_sort_realtime\n",
            "  Downloading deep_sort_realtime-1.3.2-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.13-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.12.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.59-py3-none-any.whl (906 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.8/906.8 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorthief-0.2.1-py2.py3-none-any.whl (6.1 kB)\n",
            "Downloading deep_sort_realtime-1.3.2-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.13-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: colorthief, deep_sort_realtime, ultralytics-thop, ultralytics\n",
            "Successfully installed colorthief-0.2.1 deep_sort_realtime-1.3.2 ultralytics-8.3.59 ultralytics-thop-2.0.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pc8COzQ2v4mD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrRYh75hX2Mc",
        "outputId": "dbdfd95f-9444-4650-b864-b12f255de7b5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.5.1+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aqFQcEBVv4oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.cluster import KMeans\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import  resnet152\n",
        "from PIL import Image\n",
        "import csv\n",
        "from colorthief import ColorThief\n",
        "import io\n",
        "import albumentations as A\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up GPU device with mixed precision training\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "if device == 'cuda':\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "from torchvision.models import efficientnet_b7\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class EnhancedVehicleClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedVehicleClassifier, self).__init__()\n",
        "        # Using EfficientNet-B7 as a powerful alternative\n",
        "        # B7 is the largest variant of the original EfficientNet family\n",
        "        self.features = efficientnet_b7(pretrained=True)\n",
        "        # Get the number of features from B7 model\n",
        "        num_ftrs = self.features.classifier[1].in_features\n",
        "\n",
        "        # Enhanced classifier head optimized for B7 architecture\n",
        "        self.features.classifier = nn.Sequential(\n",
        "            nn.BatchNorm1d(num_ftrs),\n",
        "            nn.Dropout(0.45),  # Higher dropout for regularization\n",
        "            nn.Linear(num_ftrs, 2560),  # Large intermediate layer\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(2560),\n",
        "            nn.Dropout(0.45),\n",
        "            nn.Linear(2560, 1280),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(1280),\n",
        "            nn.Dropout(0.35),\n",
        "            nn.Linear(1280, len(VEHICLE_SUBTYPES))\n",
        "        )\n",
        "\n",
        "        # Initialize weights using He initialization\n",
        "        for m in self.features.classifier.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        with torch.cuda.amp.autocast():\n",
        "            return self.features(x)\n",
        "\n",
        "def get_enhanced_transforms():\n",
        "    \"\"\"\n",
        "    Create enhanced image preprocessing pipeline optimized for EfficientNet-B7\n",
        "    EfficientNet-B7 expects input size of 600x600 for optimal performance\n",
        "    \"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((600, 600)),  # B7's recommended input size\n",
        "        transforms.RandomHorizontalFlip(p=0.3),\n",
        "        transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "class EnhancedColorClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnhancedColorClassifier, self).__init__()\n",
        "        # Using ResNet152 for better color feature extraction\n",
        "        self.features = resnet152(pretrained=True)\n",
        "        num_ftrs = self.features.fc.in_features\n",
        "\n",
        "        # Enhanced classifier head with attention mechanism\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 512),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.BatchNorm1d(num_ftrs),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(num_ftrs, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, len(COLOR_CLASSES))\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        with torch.cuda.amp.autocast():\n",
        "            features = self.features.conv1(x)\n",
        "            features = self.features.bn1(features)\n",
        "            features = self.features.relu(features)\n",
        "            features = self.features.maxpool(features)\n",
        "\n",
        "            features = self.features.layer1(features)\n",
        "            features = self.features.layer2(features)\n",
        "            features = self.features.layer3(features)\n",
        "            features = self.features.layer4(features)\n",
        "\n",
        "            features = self.features.avgpool(features)\n",
        "            features = torch.flatten(features, 1)\n",
        "\n",
        "            # Apply attention mechanism\n",
        "            attention_weights = self.attention(features)\n",
        "            attention_weights = torch.sigmoid(attention_weights)\n",
        "            attended_features = features * attention_weights\n",
        "\n",
        "            return self.classifier(attended_features)\n",
        "\n",
        "# Enhanced constants with more detailed categories\n",
        "VEHICLE_SUBTYPES = {\n",
        "    'car': ['sedan', 'suv', 'hatchback', 'wagon', 'coupe', 'sports_car', 'luxury', 'compact',\n",
        "            'convertible', 'crossover', 'electric_vehicle'],\n",
        "    'truck': ['pickup', 'semi', 'delivery', 'dump_truck', 'box_truck', 'flatbed', 'tanker',\n",
        "              'concrete_mixer', 'car_carrier'],\n",
        "    'bus': ['city_bus', 'coach', 'mini_bus', 'school_bus', 'articulated_bus', 'double_decker',\n",
        "            'shuttle_bus'],\n",
        "    'van': ['passenger_van', 'cargo_van', 'minivan', 'camper_van', 'panel_van', 'refrigerated_van',\n",
        "            'step_van'],\n",
        "    'two_wheeler': ['motorcycle', 'scooter', 'bicycle', 'electric_bike', 'moped', 'sport_bike',\n",
        "                   'cruiser', 'touring_bike']\n",
        "}\n",
        "\n",
        "COLOR_CLASSES = [\n",
        "    'black', 'white', 'gray', 'silver', 'red', 'blue', 'green', 'yellow',\n",
        "    'brown', 'orange', 'purple', 'gold', 'beige', 'burgundy', 'navy',\n",
        "    'teal', 'bronze', 'copper', 'champagne'\n",
        "]\n",
        "\n",
        "def get_enhanced_transforms():\n",
        "    \"\"\"\n",
        "    Create enhanced image preprocessing pipeline with augmentations\n",
        "    \"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((384, 384)),  # Larger input size for better detail capture\n",
        "        transforms.RandomHorizontalFlip(p=0.3),\n",
        "        transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "def classify_vehicle_details(frame, box, class_name, vehicle_classifier, confidence_threshold=0.7):\n",
        "    \"\"\"\n",
        "    Enhanced vehicle classification with confidence thresholding and error handling\n",
        "    \"\"\"\n",
        "    try:\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "        vehicle_img = frame[y1:y2, x1:x2]\n",
        "\n",
        "        # Check if image patch is valid\n",
        "        if vehicle_img.size == 0 or vehicle_img.shape[0] == 0 or vehicle_img.shape[1] == 0:\n",
        "            return None\n",
        "\n",
        "        # Convert to PIL and apply enhanced preprocessing\n",
        "        img_pil = Image.fromarray(cv2.cvtColor(vehicle_img, cv2.COLOR_BGR2RGB))\n",
        "        transform = get_enhanced_transforms()\n",
        "        input_tensor = transform(img_pil).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = vehicle_classifier(input_tensor)\n",
        "                probabilities = torch.softmax(outputs, dim=1)\n",
        "                conf, predicted = probabilities.max(1)\n",
        "\n",
        "                # Only classify if confidence exceeds threshold\n",
        "                if conf.item() < confidence_threshold:\n",
        "                    return None\n",
        "\n",
        "        subtype = list(VEHICLE_SUBTYPES[class_name])[predicted.item() % len(VEHICLE_SUBTYPES[class_name])]\n",
        "\n",
        "        # Enhanced size classification using relative dimensions\n",
        "        area = (x2 - x1) * (y2 - y1)\n",
        "        frame_area = frame.shape[0] * frame.shape[1]\n",
        "        area_ratio = area / frame_area\n",
        "\n",
        "        size = 'small' if area_ratio < 0.05 else \\\n",
        "               'medium' if area_ratio < 0.15 else 'large'\n",
        "\n",
        "        # Enhanced body style and purpose classification\n",
        "        body_style = 'commercial' if subtype in ['semi', 'delivery', 'dump_truck', 'box_truck', 'tanker',\n",
        "                                               'concrete_mixer', 'car_carrier', 'refrigerated_van'] else \\\n",
        "                    'passenger' if subtype in ['city_bus', 'school_bus', 'coach', 'shuttle_bus'] else 'standard'\n",
        "\n",
        "        purpose = 'commercial' if body_style == 'commercial' else \\\n",
        "                 'public' if body_style == 'passenger' else 'personal'\n",
        "\n",
        "        return {\n",
        "            'subtype': subtype,\n",
        "            'size': size,\n",
        "            'body_style': body_style,\n",
        "            'purpose': purpose,\n",
        "            'confidence': conf.item()\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Vehicle classification error: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def setup_models():\n",
        "    \"\"\"\n",
        "    Initialize enhanced models with EfficientNet-B7\n",
        "    \"\"\"\n",
        "    # Initialize YOLOv8x with enhanced parameters\n",
        "    yolo_model = YOLO('yolov8x.pt')\n",
        "    yolo_model.conf = 0.35\n",
        "    yolo_model.iou = 0.65\n",
        "    yolo_model.to(device)\n",
        "\n",
        "    # Initialize enhanced classifier with B7 model\n",
        "    vehicle_classifier = EnhancedVehicleClassifier().to(device)\n",
        "    vehicle_classifier.eval()\n",
        "\n",
        "    color_classifier = EnhancedColorClassifier().to(device)\n",
        "    color_classifier.eval()\n",
        "\n",
        "    # Initialize DeepSort with optimized parameters\n",
        "    tracker = DeepSort(\n",
        "        max_age=45,\n",
        "        n_init=4,\n",
        "        nms_max_overlap=0.85,\n",
        "        max_cosine_distance=0.25,\n",
        "        nn_budget=150,\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'yolo': yolo_model,\n",
        "        'vehicle_classifier': vehicle_classifier,\n",
        "        'color_classifier': color_classifier,\n",
        "        'tracker': tracker,\n",
        "        'device': device\n",
        "    }\n",
        "def detect_color(frame, box, color_classifier, confidence_threshold=0.6):\n",
        "    \"\"\"\n",
        "    Enhanced color detection with multi-method approach and confidence thresholding\n",
        "    \"\"\"\n",
        "    try:\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "        vehicle_img = frame[y1:y2, x1:x2]\n",
        "\n",
        "        if vehicle_img.size == 0:\n",
        "            return None\n",
        "\n",
        "        # Convert to PIL Image\n",
        "        img_pil = Image.fromarray(cv2.cvtColor(vehicle_img, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        # Enhanced color detection using multiple methods\n",
        "        results = {'colors': [], 'confidences': []}\n",
        "\n",
        "        # Method 1: ML Classification\n",
        "        transform = get_enhanced_transforms()\n",
        "        input_tensor = transform(img_pil).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = color_classifier(input_tensor)\n",
        "                probabilities = torch.softmax(outputs, dim=1)\n",
        "                conf, predicted = probabilities.max(1)\n",
        "\n",
        "                if conf.item() >= confidence_threshold:\n",
        "                    results['colors'].append(COLOR_CLASSES[predicted.item()])\n",
        "                    results['confidences'].append(conf.item())\n",
        "\n",
        "        # Method 2: ColorThief analysis\n",
        "        img_byte_arr = io.BytesIO()\n",
        "        img_pil.save(img_byte_arr, format='PNG')\n",
        "        img_byte_arr.seek(0)\n",
        "\n",
        "        color_thief = ColorThief(img_byte_arr)\n",
        "        dominant_color = color_thief.get_color(quality=1)\n",
        "        palette = color_thief.get_palette(color_count=3, quality=1)\n",
        "\n",
        "        # Convert RGB values to color names using nearest neighbor\n",
        "        for rgb in [dominant_color] + palette:\n",
        "            color_name = get_closest_color(rgb)\n",
        "            if color_name not in results['colors']:\n",
        "                results['colors'].append(color_name)\n",
        "                results['confidences'].append(0.8)  # Default confidence for color matching\n",
        "\n",
        "        # Return most confident color\n",
        "        if results['colors']:\n",
        "            max_conf_idx = np.argmax(results['confidences'])\n",
        "            return {\n",
        "                'color': results['colors'][max_conf_idx],\n",
        "                'confidence': results['confidences'][max_conf_idx],\n",
        "                'secondary_colors': results['colors'][:3]  # Top 3 detected colors\n",
        "            }\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Color detection error: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_closest_color(rgb):\n",
        "    \"\"\"\n",
        "    Convert RGB value to the closest named color using an extended color mapping.\n",
        "    \"\"\"\n",
        "    color_map = {\n",
        "        'black': (0, 0, 0),\n",
        "        'white': (255, 255, 255),\n",
        "        'gray': (128, 128, 128),\n",
        "        'silver': (192, 192, 192),\n",
        "        'red': (255, 0, 0),\n",
        "        'blue': (0, 0, 255),\n",
        "        'green': (0, 255, 0),\n",
        "        'yellow': (255, 255, 0),\n",
        "        'brown': (165, 42, 42),\n",
        "        'orange': (255, 165, 0),\n",
        "        'purple': (128, 0, 128),\n",
        "        'gold': (255, 215, 0),\n",
        "        'beige': (245, 245, 220),\n",
        "        'burgundy': (128, 0, 32),\n",
        "        'navy': (0, 0, 128),\n",
        "        'teal': (0, 128, 128),\n",
        "        'bronze': (205, 127, 50),\n",
        "        'copper': (184, 115, 51),\n",
        "        'champagne': (247, 231, 206),\n",
        "        'pink': (255, 192, 203),\n",
        "        'cyan': (0, 255, 255),\n",
        "        'lime': (191, 255, 0),\n",
        "        'magenta': (255, 0, 255),\n",
        "        'olive': (128, 128, 0),\n",
        "        'peach': (255, 229, 180),\n",
        "    }\n",
        "\n",
        "    min_distance = float('inf')\n",
        "    closest_color = None\n",
        "\n",
        "    for color_name, color_rgb in color_map.items():\n",
        "        distance = sum((a - b) ** 2 for a, b in zip(rgb, color_rgb))\n",
        "        if distance < min_distance:\n",
        "            min_distance = distance\n",
        "            closest_color = color_name\n",
        "\n",
        "    return closest_color\n",
        "\n",
        "\n",
        "    for color_name, color_rgb in color_map.items():\n",
        "        distance = sum((a - b) ** 2 for a, b in zip(rgb, color_rgb))\n",
        "        if distance < min_distance:\n",
        "            min_distance = distance\n",
        "            closest_color = color_name\n",
        "\n",
        "    return closest_color\n",
        "\n",
        "def process_video(video_path, output_path, models, data_output_path):\n",
        "    \"\"\"\n",
        "    Enhanced video processing with multi-frame analysis and temporal smoothing\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(f\"Could not open video: {video_path}\")\n",
        "\n",
        "    # Video writer setup\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
        "\n",
        "    data = []\n",
        "    frame_count = 0\n",
        "    temporal_buffer = {}  # Store recent detections for temporal smoothing\n",
        "\n",
        "    try:\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            frame_count += 1\n",
        "\n",
        "            # Process every frame for maximum accuracy\n",
        "            detection_list = []\n",
        "\n",
        "            # Run YOLO detection with test-time augmentation\n",
        "            results = models['yolo'](frame, augment=True)  # Enable test-time augmentation\n",
        "\n",
        "            # Process YOLO detections\n",
        "            for box, conf, cls in zip(results[0].boxes.xyxy, results[0].boxes.conf, results[0].boxes.cls):\n",
        "                x1, y1, x2, y2 = map(int, box.tolist())\n",
        "                detection = ([x1, y1, x2 - x1, y2 - y1], conf.item(), int(cls.item()))\n",
        "                detection_list.append(detection)\n",
        "\n",
        "            # Update tracks with enhanced DeepSort\n",
        "            tracks = models['tracker'].update_tracks(detection_list, frame=frame)\n",
        "\n",
        "            # Process each track with temporal smoothing\n",
        "            for track in tracks:\n",
        "                if not track.is_confirmed():\n",
        "                    continue\n",
        "\n",
        "                track_id = track.track_id\n",
        "                ltwh = track\n",
        "                to_ltwh = track.to_ltwh()\n",
        "                box = [int(to_ltwh[0]), int(to_ltwh[1]),\n",
        "                      int(to_ltwh[0] + to_ltwh[2]), int(to_ltwh[1] + to_ltwh[3])]\n",
        "\n",
        "                cls_id = track.get_det_class()\n",
        "                if cls_id is None:\n",
        "                    continue\n",
        "\n",
        "                class_name = models['yolo'].names[cls_id]\n",
        "\n",
        "                if class_name in VEHICLE_SUBTYPES:\n",
        "                    # Initialize temporal buffer for this track if needed\n",
        "                    if track_id not in temporal_buffer:\n",
        "                        temporal_buffer[track_id] = {\n",
        "                            'vehicle_details': [],\n",
        "                            'color_info': [],\n",
        "                            'frame_history': []\n",
        "                        }\n",
        "\n",
        "                    # Get vehicle details\n",
        "                    vehicle_details = classify_vehicle_details(\n",
        "                        frame, box, class_name,\n",
        "                        models['vehicle_classifier']\n",
        "                    )\n",
        "\n",
        "                    # Get color information\n",
        "                    color_info = detect_color(\n",
        "                        frame, box,\n",
        "                        models['color_classifier']\n",
        "                    )\n",
        "\n",
        "                    # Update temporal buffer\n",
        "                    if vehicle_details and color_info:\n",
        "                        temporal_buffer[track_id]['vehicle_details'].append(vehicle_details)\n",
        "                        temporal_buffer[track_id]['color_info'].append(color_info)\n",
        "                        temporal_buffer[track_id]['frame_history'].append(frame_count)\n",
        "\n",
        "                        # Keep only recent history (last 5 frames)\n",
        "                        max_history = 5\n",
        "                        if len(temporal_buffer[track_id]['vehicle_details']) > max_history:\n",
        "                            temporal_buffer[track_id]['vehicle_details'] = temporal_buffer[track_id]['vehicle_details'][-max_history:]\n",
        "                            temporal_buffer[track_id]['color_info'] = temporal_buffer[track_id]['color_info'][-max_history:]\n",
        "                            temporal_buffer[track_id]['frame_history'] = temporal_buffer[track_id]['frame_history'][-max_history:]\n",
        "\n",
        "                        # Get smoothed predictions using temporal buffer\n",
        "                        smoothed_details = get_smoothed_predictions(temporal_buffer[track_id])\n",
        "\n",
        "                        # Create detailed label with confidence scores\n",
        "                        label_parts = [\n",
        "                            f\"{smoothed_details['subtype']} ({smoothed_details['size']})\",\n",
        "                            f\"#{track_id}\",\n",
        "                            f\"{smoothed_details['color']}\",\n",
        "                            f\"{smoothed_details['confidence']:.2f}\"\n",
        "                        ]\n",
        "                        label = \" \".join(label_parts)\n",
        "\n",
        "                        # Calculate dynamic color based on confidence\n",
        "                        confidence = smoothed_details['confidence']\n",
        "                        bbox_color = (\n",
        "                            int(255 * (1 - confidence)),  # Red component\n",
        "                            int(255 * confidence),        # Green component\n",
        "                            0                            # Blue component\n",
        "                        )\n",
        "\n",
        "                        # Enhanced visualization\n",
        "                        cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), bbox_color, 2)\n",
        "\n",
        "                        # Add background to text for better visibility\n",
        "                        text_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]\n",
        "                        cv2.rectangle(frame,\n",
        "                                    (box[0], box[1] - 25),\n",
        "                                    (box[0] + text_size[0], box[1]),\n",
        "                                    bbox_color, -1)\n",
        "                        cv2.putText(frame, label, (box[0], box[1] - 10),\n",
        "                                  cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
        "\n",
        "                        # Store detection data with enhanced information\n",
        "                        data.append({\n",
        "                            'Frame': frame_count,\n",
        "                            'Track_ID': track_id,\n",
        "                            'Main_Type': class_name,\n",
        "                            'Subtype': smoothed_details['subtype'],\n",
        "                            'Size': smoothed_details['size'],\n",
        "                            'Body_Style': smoothed_details['body_style'],\n",
        "                            'Purpose': smoothed_details['purpose'],\n",
        "                            'Primary_Color': smoothed_details['color'],\n",
        "                            'Secondary_Colors': smoothed_details['secondary_colors'],\n",
        "                            'Detection_Confidence': track.get_det_conf() or 0.0,\n",
        "                            'Classification_Confidence': smoothed_details['confidence'],\n",
        "                            'Box_Coordinates': box,\n",
        "                            'Temporal_Confidence': len(temporal_buffer[track_id]['vehicle_details']) / max_history\n",
        "                        })\n",
        "\n",
        "            # Clean up old tracks from temporal buffer\n",
        "            current_track_ids = {track.track_id for track in tracks if track.is_confirmed()}\n",
        "            temporal_buffer = {k: v for k, v in temporal_buffer.items() if k in current_track_ids}\n",
        "\n",
        "            # Add frame counter and processing statistics\n",
        "            cv2.putText(frame, f\"Frame: {frame_count}\", (10, 30),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "\n",
        "            out.write(frame)\n",
        "\n",
        "    finally:\n",
        "        # Release resources\n",
        "        cap.release()\n",
        "        out.release()\n",
        "\n",
        "        # Save detection data with enhanced error handling\n",
        "        if data:\n",
        "            try:\n",
        "                with open(data_output_path, 'w', newline='') as f:\n",
        "                    writer = csv.DictWriter(f, fieldnames=data[0].keys())\n",
        "                    writer.writeheader()\n",
        "                    writer.writerows(data)\n",
        "                print(f\"Successfully saved detection data to {data_output_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error saving detection data: {e}\")\n",
        "                # Attempt to save to alternative location\n",
        "                backup_path = \"backup_\" + data_output_path\n",
        "                with open(backup_path, 'w', newline='') as f:\n",
        "                    writer = csv.DictWriter(f, fieldnames=data[0].keys())\n",
        "                    writer.writeheader()\n",
        "                    writer.writerows(data)\n",
        "                print(f\"Saved backup detection data to {backup_path}\")\n",
        "\n",
        "def get_smoothed_predictions(track_buffer):\n",
        "    \"\"\"\n",
        "    Calculate smoothed predictions using temporal buffer data\n",
        "    \"\"\"\n",
        "    # Get recent vehicle details\n",
        "    recent_details = track_buffer['vehicle_details']\n",
        "    recent_colors = track_buffer['color_info']\n",
        "\n",
        "    if not recent_details or not recent_colors:\n",
        "        return None\n",
        "\n",
        "    # Count occurrences of each prediction\n",
        "    subtype_counts = Counter(d['subtype'] for d in recent_details)\n",
        "    size_counts = Counter(d['size'] for d in recent_details)\n",
        "    body_style_counts = Counter(d['body_style'] for d in recent_details)\n",
        "    purpose_counts = Counter(d['purpose'] for d in recent_details)\n",
        "    color_counts = Counter(c['color'] for c in recent_colors)\n",
        "\n",
        "    # Calculate average confidence\n",
        "    avg_confidence = np.mean([d['confidence'] for d in recent_details])\n",
        "\n",
        "    # Get secondary colors from recent detections\n",
        "    all_secondary_colors = []\n",
        "    for color_info in recent_colors:\n",
        "        if 'secondary_colors' in color_info:\n",
        "            all_secondary_colors.extend(color_info['secondary_colors'])\n",
        "    secondary_colors = [color for color, count in Counter(all_secondary_colors).most_common(3)]\n",
        "\n",
        "    # Return smoothed predictions\n",
        "    return {\n",
        "        'subtype': subtype_counts.most_common(1)[0][0],\n",
        "        'size': size_counts.most_common(1)[0][0],\n",
        "        'body_style': body_style_counts.most_common(1)[0][0],\n",
        "        'purpose': purpose_counts.most_common(1)[0][0],\n",
        "        'color': color_counts.most_common(1)[0][0],\n",
        "        'secondary_colors': secondary_colors,\n",
        "        'confidence': avg_confidence\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function with enhanced error handling and logging\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Define paths\n",
        "        video_path = \"/content/WhatsApp Video 2025-01-03 at 01.11.52_2850e0bf (1).mp4\"\n",
        "        output_path = \"output_video.mp4\"\n",
        "        data_output_path = \"vehicle_tracking_data.csv\"\n",
        "\n",
        "        print(\"Initializing models...\")\n",
        "        models = setup_models()\n",
        "        print(\"Models initialized successfully\")\n",
        "\n",
        "        print(\"Starting video processing...\")\n",
        "        process_video(video_path, output_path, models, data_output_path)\n",
        "        print(\"Video processing completed successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main execution: {e}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YENalhwTdHb",
        "outputId": "80de7daf-251d-4077-b2c8-ca394bc488bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Using device: cuda\n",
            "Initializing models...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8x.pt to 'yolov8x.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "100%|██████████| 131M/131M [00:00<00:00, 235MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b7_lukemelas-c5b4e57e.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b7_lukemelas-c5b4e57e.pth\n",
            "100%|██████████| 255M/255M [00:02<00:00, 130MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n",
            "100%|██████████| 230M/230M [00:01<00:00, 161MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models initialized successfully\n",
            "Starting video processing...\n",
            "\n",
            "0: 384x640 4 persons, 13 cars, 1 motorcycle, 1 truck, 7630.1ms\n",
            "Speed: 10.2ms preprocess, 7630.1ms inference, 608.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 11 cars, 1 motorcycle, 2 buss, 87.4ms\n",
            "Speed: 5.9ms preprocess, 87.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 11 cars, 1 motorcycle, 3 buss, 1 truck, 85.7ms\n",
            "Speed: 4.3ms preprocess, 85.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 10 cars, 1 motorcycle, 2 buss, 1 truck, 86.6ms\n",
            "Speed: 4.8ms preprocess, 86.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 11 cars, 1 motorcycle, 3 buss, 1 truck, 127.8ms\n",
            "Speed: 3.9ms preprocess, 127.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 9 cars, 1 motorcycle, 2 buss, 1 truck, 111.7ms\n",
            "Speed: 3.3ms preprocess, 111.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 15 cars, 1 motorcycle, 2 buss, 1 truck, 112.4ms\n",
            "Speed: 2.9ms preprocess, 112.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 11 cars, 1 motorcycle, 1 bus, 1 truck, 153.0ms\n",
            "Speed: 3.2ms preprocess, 153.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 11 cars, 1 motorcycle, 2 buss, 1 truck, 139.2ms\n",
            "Speed: 2.8ms preprocess, 139.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 13 cars, 1 motorcycle, 1 bus, 1 truck, 139.1ms\n",
            "Speed: 5.3ms preprocess, 139.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7 cars, 1 motorcycle, 2 buss, 1 truck, 155.9ms\n",
            "Speed: 5.7ms preprocess, 155.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 12 cars, 1 motorcycle, 1 bus, 1 truck, 130.2ms\n",
            "Speed: 3.0ms preprocess, 130.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 10 cars, 1 motorcycle, 1 bus, 1 truck, 136.3ms\n",
            "Speed: 3.1ms preprocess, 136.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 11 cars, 1 motorcycle, 2 buss, 1 truck, 128.2ms\n",
            "Speed: 4.1ms preprocess, 128.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 12 cars, 1 motorcycle, 2 buss, 1 truck, 138.7ms\n",
            "Speed: 2.9ms preprocess, 138.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 14 cars, 1 motorcycle, 2 buss, 1 truck, 137.7ms\n",
            "Speed: 2.4ms preprocess, 137.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 13 cars, 1 motorcycle, 2 buss, 1 truck, 163.1ms\n",
            "Speed: 3.8ms preprocess, 163.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 13 cars, 1 motorcycle, 4 buss, 1 truck, 122.2ms\n",
            "Speed: 4.0ms preprocess, 122.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 15 cars, 1 motorcycle, 1 bus, 116.3ms\n",
            "Speed: 2.8ms preprocess, 116.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 13 cars, 1 motorcycle, 2 buss, 137.8ms\n",
            "Speed: 2.5ms preprocess, 137.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 15 cars, 1 motorcycle, 3 buss, 1 truck, 97.4ms\n",
            "Speed: 3.9ms preprocess, 97.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 13 cars, 1 motorcycle, 1 bus, 1 truck, 136.4ms\n",
            "Speed: 3.5ms preprocess, 136.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 15 cars, 1 motorcycle, 3 buss, 1 truck, 131.4ms\n",
            "Speed: 3.6ms preprocess, 131.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 7 persons, 13 cars, 1 motorcycle, 2 buss, 1 truck, 136.0ms\n",
            "Speed: 3.2ms preprocess, 136.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 12 cars, 1 motorcycle, 4 buss, 138.2ms\n",
            "Speed: 4.0ms preprocess, 138.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 12 cars, 1 motorcycle, 3 buss, 135.0ms\n",
            "Speed: 5.2ms preprocess, 135.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 12 cars, 1 motorcycle, 2 buss, 1 truck, 112.8ms\n",
            "Speed: 5.5ms preprocess, 112.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 12 cars, 1 motorcycle, 1 bus, 1 truck, 108.0ms\n",
            "Speed: 2.3ms preprocess, 108.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 11 cars, 1 motorcycle, 1 bus, 1 truck, 135.9ms\n",
            "Speed: 5.2ms preprocess, 135.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 12 cars, 1 motorcycle, 2 buss, 1 truck, 154.5ms\n",
            "Speed: 3.0ms preprocess, 154.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 13 cars, 1 motorcycle, 1 bus, 1 truck, 134.0ms\n",
            "Speed: 2.7ms preprocess, 134.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 13 cars, 3 buss, 2 trucks, 151.2ms\n",
            "Speed: 4.8ms preprocess, 151.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 14 cars, 3 buss, 1 truck, 124.2ms\n",
            "Speed: 4.2ms preprocess, 124.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 16 cars, 2 buss, 1 truck, 130.4ms\n",
            "Speed: 2.3ms preprocess, 130.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 16 cars, 2 buss, 2 trucks, 142.5ms\n",
            "Speed: 4.9ms preprocess, 142.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 16 cars, 4 buss, 2 trucks, 158.4ms\n",
            "Speed: 3.4ms preprocess, 158.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 16 cars, 2 buss, 1 truck, 133.4ms\n",
            "Speed: 3.3ms preprocess, 133.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 16 cars, 3 buss, 1 truck, 139.1ms\n",
            "Speed: 4.5ms preprocess, 139.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 15 cars, 2 buss, 1 truck, 138.6ms\n",
            "Speed: 3.3ms preprocess, 138.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 13 cars, 4 buss, 1 truck, 136.4ms\n",
            "Speed: 4.4ms preprocess, 136.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 14 cars, 3 buss, 1 truck, 136.2ms\n",
            "Speed: 2.3ms preprocess, 136.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 14 cars, 3 buss, 1 truck, 106.1ms\n",
            "Speed: 4.1ms preprocess, 106.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 12 cars, 3 buss, 1 truck, 98.3ms\n",
            "Speed: 5.0ms preprocess, 98.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 14 cars, 3 buss, 104.4ms\n",
            "Speed: 2.8ms preprocess, 104.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 14 cars, 2 buss, 1 truck, 95.3ms\n",
            "Speed: 2.4ms preprocess, 95.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 14 cars, 2 buss, 121.3ms\n",
            "Speed: 5.0ms preprocess, 121.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 13 cars, 2 buss, 140.8ms\n",
            "Speed: 3.8ms preprocess, 140.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 14 cars, 2 buss, 101.8ms\n",
            "Speed: 2.9ms preprocess, 101.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 14 cars, 2 buss, 100.5ms\n",
            "Speed: 2.4ms preprocess, 100.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 11 cars, 2 buss, 138.1ms\n",
            "Speed: 4.1ms preprocess, 138.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 13 cars, 2 buss, 1 truck, 1 parking meter, 106.3ms\n",
            "Speed: 2.3ms preprocess, 106.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 12 cars, 1 bus, 98.6ms\n",
            "Speed: 3.2ms preprocess, 98.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 14 cars, 1 bus, 95.7ms\n",
            "Speed: 2.5ms preprocess, 95.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 12 cars, 1 bus, 101.9ms\n",
            "Speed: 4.7ms preprocess, 101.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 11 cars, 1 bus, 140.6ms\n",
            "Speed: 4.1ms preprocess, 140.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 14 cars, 1 bus, 1 truck, 102.1ms\n",
            "Speed: 3.0ms preprocess, 102.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 12 cars, 1 bus, 135.6ms\n",
            "Speed: 3.5ms preprocess, 135.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 13 cars, 1 bus, 136.5ms\n",
            "Speed: 2.7ms preprocess, 136.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 15 cars, 1 bus, 114.3ms\n",
            "Speed: 2.9ms preprocess, 114.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 14 cars, 1 bus, 140.8ms\n",
            "Speed: 3.3ms preprocess, 140.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 17 cars, 1 bus, 146.9ms\n",
            "Speed: 3.1ms preprocess, 146.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 16 cars, 1 bus, 134.2ms\n",
            "Speed: 3.0ms preprocess, 134.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 16 cars, 1 bus, 142.8ms\n",
            "Speed: 2.5ms preprocess, 142.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 15 cars, 1 bus, 127.1ms\n",
            "Speed: 3.0ms preprocess, 127.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 15 cars, 1 bus, 135.9ms\n",
            "Speed: 3.5ms preprocess, 135.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 13 cars, 1 bus, 126.6ms\n",
            "Speed: 6.3ms preprocess, 126.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 13 cars, 1 bus, 125.6ms\n",
            "Speed: 2.9ms preprocess, 125.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 13 cars, 1 bus, 130.7ms\n",
            "Speed: 4.3ms preprocess, 130.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 14 cars, 1 bus, 127.1ms\n",
            "Speed: 4.1ms preprocess, 127.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 16 cars, 1 bus, 96.8ms\n",
            "Speed: 4.6ms preprocess, 96.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 13 cars, 117.2ms\n",
            "Speed: 2.8ms preprocess, 117.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 15 cars, 139.5ms\n",
            "Speed: 3.8ms preprocess, 139.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 16 cars, 135.5ms\n",
            "Speed: 3.2ms preprocess, 135.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 14 cars, 136.7ms\n",
            "Speed: 3.0ms preprocess, 136.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 14 cars, 132.3ms\n",
            "Speed: 2.3ms preprocess, 132.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 13 cars, 143.2ms\n",
            "Speed: 2.2ms preprocess, 143.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 17 cars, 1 bus, 131.4ms\n",
            "Speed: 3.9ms preprocess, 131.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 18 cars, 1 bus, 138.4ms\n",
            "Speed: 2.3ms preprocess, 138.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 17 cars, 1 bus, 132.3ms\n",
            "Speed: 3.1ms preprocess, 132.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 16 cars, 1 bus, 138.0ms\n",
            "Speed: 3.2ms preprocess, 138.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 14 cars, 1 bus, 133.1ms\n",
            "Speed: 3.2ms preprocess, 133.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 15 cars, 1 bus, 136.3ms\n",
            "Speed: 3.1ms preprocess, 136.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 15 cars, 1 bus, 138.3ms\n",
            "Speed: 3.0ms preprocess, 138.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 15 cars, 1 bus, 136.3ms\n",
            "Speed: 2.4ms preprocess, 136.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 17 cars, 1 bus, 136.3ms\n",
            "Speed: 3.3ms preprocess, 136.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 16 cars, 1 bus, 117.8ms\n",
            "Speed: 3.1ms preprocess, 117.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 18 cars, 1 bus, 123.4ms\n",
            "Speed: 2.7ms preprocess, 123.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 18 cars, 1 bus, 115.5ms\n",
            "Speed: 2.9ms preprocess, 115.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 18 cars, 1 bus, 108.4ms\n",
            "Speed: 3.9ms preprocess, 108.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 16 cars, 1 bus, 95.3ms\n",
            "Speed: 3.1ms preprocess, 95.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 18 cars, 1 bus, 122.1ms\n",
            "Speed: 2.9ms preprocess, 122.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 15 cars, 1 bus, 92.8ms\n",
            "Speed: 2.8ms preprocess, 92.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 16 cars, 1 bus, 117.6ms\n",
            "Speed: 2.9ms preprocess, 117.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 14 cars, 1 bus, 133.6ms\n",
            "Speed: 3.2ms preprocess, 133.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 15 cars, 1 bus, 97.7ms\n",
            "Speed: 3.0ms preprocess, 97.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 15 cars, 1 bus, 138.2ms\n",
            "Speed: 3.1ms preprocess, 138.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 13 cars, 1 bus, 103.9ms\n",
            "Speed: 2.4ms preprocess, 103.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 13 cars, 1 bus, 102.3ms\n",
            "Speed: 2.9ms preprocess, 102.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 16 cars, 1 bus, 98.1ms\n",
            "Speed: 3.7ms preprocess, 98.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 17 cars, 1 bus, 135.7ms\n",
            "Speed: 2.9ms preprocess, 135.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 20 cars, 122.3ms\n",
            "Speed: 4.2ms preprocess, 122.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 17 cars, 1 bus, 112.3ms\n",
            "Speed: 4.4ms preprocess, 112.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 16 cars, 118.1ms\n",
            "Speed: 3.7ms preprocess, 118.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 17 cars, 125.5ms\n",
            "Speed: 5.3ms preprocess, 125.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 14 cars, 100.9ms\n",
            "Speed: 2.9ms preprocess, 100.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 14 cars, 165.6ms\n",
            "Speed: 3.2ms preprocess, 165.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 16 cars, 104.7ms\n",
            "Speed: 4.8ms preprocess, 104.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 18 cars, 149.4ms\n",
            "Speed: 4.5ms preprocess, 149.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 19 cars, 115.2ms\n",
            "Speed: 3.0ms preprocess, 115.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 16 cars, 113.7ms\n",
            "Speed: 3.6ms preprocess, 113.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 15 cars, 137.9ms\n",
            "Speed: 4.1ms preprocess, 137.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 15 cars, 124.5ms\n",
            "Speed: 2.7ms preprocess, 124.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 15 cars, 136.5ms\n",
            "Speed: 2.9ms preprocess, 136.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 18 cars, 120.4ms\n",
            "Speed: 2.3ms preprocess, 120.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 16 cars, 124.1ms\n",
            "Speed: 3.8ms preprocess, 124.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 13 cars, 136.2ms\n",
            "Speed: 4.2ms preprocess, 136.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 13 cars, 107.1ms\n",
            "Speed: 4.2ms preprocess, 107.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 13 cars, 103.1ms\n",
            "Speed: 2.4ms preprocess, 103.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 cars, 131.0ms\n",
            "Speed: 3.2ms preprocess, 131.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 15 cars, 138.9ms\n",
            "Speed: 6.0ms preprocess, 138.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 10 cars, 136.0ms\n",
            "Speed: 3.4ms preprocess, 136.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 12 cars, 136.6ms\n",
            "Speed: 2.2ms preprocess, 136.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 139.3ms\n",
            "Speed: 3.9ms preprocess, 139.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 138.6ms\n",
            "Speed: 3.7ms preprocess, 138.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 139.3ms\n",
            "Speed: 5.0ms preprocess, 139.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 141.4ms\n",
            "Speed: 3.9ms preprocess, 141.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 121.2ms\n",
            "Speed: 3.8ms preprocess, 121.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 9 cars, 107.9ms\n",
            "Speed: 5.1ms preprocess, 107.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 107.5ms\n",
            "Speed: 3.2ms preprocess, 107.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 108.4ms\n",
            "Speed: 4.4ms preprocess, 108.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 10 cars, 135.3ms\n",
            "Speed: 4.2ms preprocess, 135.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 9 cars, 98.6ms\n",
            "Speed: 2.9ms preprocess, 98.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 8 cars, 115.5ms\n",
            "Speed: 4.3ms preprocess, 115.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 11 cars, 163.9ms\n",
            "Speed: 3.2ms preprocess, 163.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 12 cars, 97.9ms\n",
            "Speed: 3.8ms preprocess, 97.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 10 cars, 102.1ms\n",
            "Speed: 3.6ms preprocess, 102.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 11 cars, 1 bus, 128.7ms\n",
            "Speed: 3.1ms preprocess, 128.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 11 cars, 99.6ms\n",
            "Speed: 2.7ms preprocess, 99.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 10 cars, 1 bus, 132.9ms\n",
            "Speed: 2.3ms preprocess, 132.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 11 cars, 1 bus, 94.2ms\n",
            "Speed: 2.9ms preprocess, 94.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 1 bicycle, 7 cars, 1 bus, 136.7ms\n",
            "Speed: 4.3ms preprocess, 136.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 8 cars, 1 bus, 102.2ms\n",
            "Speed: 3.7ms preprocess, 102.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7 cars, 1 bus, 123.3ms\n",
            "Speed: 4.1ms preprocess, 123.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 9 cars, 1 bus, 139.9ms\n",
            "Speed: 3.9ms preprocess, 139.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7 cars, 154.7ms\n",
            "Speed: 3.5ms preprocess, 154.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 7 cars, 1 bus, 129.0ms\n",
            "Speed: 2.8ms preprocess, 129.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 10 cars, 1 bus, 93.5ms\n",
            "Speed: 2.7ms preprocess, 93.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 11 cars, 1 bus, 91.6ms\n",
            "Speed: 3.9ms preprocess, 91.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 12 cars, 1 bench, 95.5ms\n",
            "Speed: 2.9ms preprocess, 95.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 11 cars, 147.7ms\n",
            "Speed: 3.0ms preprocess, 147.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 11 cars, 161.2ms\n",
            "Speed: 2.2ms preprocess, 161.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 10 cars, 99.1ms\n",
            "Speed: 4.9ms preprocess, 99.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 12 cars, 145.1ms\n",
            "Speed: 2.9ms preprocess, 145.1ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 10 cars, 92.2ms\n",
            "Speed: 3.7ms preprocess, 92.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 10 cars, 144.4ms\n",
            "Speed: 3.1ms preprocess, 144.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 10 cars, 98.5ms\n",
            "Speed: 3.6ms preprocess, 98.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 10 cars, 145.0ms\n",
            "Speed: 6.1ms preprocess, 145.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 10 cars, 146.0ms\n",
            "Speed: 2.3ms preprocess, 146.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 11 cars, 145.1ms\n",
            "Speed: 3.7ms preprocess, 145.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 10 cars, 111.8ms\n",
            "Speed: 3.6ms preprocess, 111.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 9 cars, 1 bus, 92.0ms\n",
            "Speed: 2.2ms preprocess, 92.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 10 cars, 97.3ms\n",
            "Speed: 2.9ms preprocess, 97.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 10 cars, 93.5ms\n",
            "Speed: 3.6ms preprocess, 93.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 9 cars, 146.6ms\n",
            "Speed: 4.7ms preprocess, 146.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7 cars, 96.7ms\n",
            "Speed: 3.1ms preprocess, 96.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 person, 7 cars, 136.7ms\n",
            "Speed: 3.2ms preprocess, 136.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 2 persons, 6 cars, 146.8ms\n",
            "Speed: 2.9ms preprocess, 146.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Successfully saved detection data to vehicle_tracking_data.csv\n",
            "Video processing completed successfully\n"
          ]
        }
      ]
    }
  ]
}